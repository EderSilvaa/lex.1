# Implementa√ß√£o de Streaming - LEX Agent

## Resumo

Implementa√ß√£o completa de **streaming de respostas em tempo real** da IA, permitindo que o usu√°rio veja a resposta palavra por palavra conforme √© gerada, similar ao ChatGPT.

## Arquitetura

### 1. Edge Function (Servidor)
**Arquivo**: [EDGE-FUNCTION-OPENIA-STREAMING.ts](EDGE-FUNCTION-OPENIA-STREAMING.ts)

A Edge Function foi modificada para retornar **Server-Sent Events (SSE)** ao inv√©s de JSON:

```typescript
// Ativar streaming na API OpenAI
body: JSON.stringify({
  model: 'gpt-4o',
  messages: [...],
  stream: true  // ‚Üê Ativa streaming
})

// Retornar como text/event-stream
return new Response(stream, {
  headers: {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
  }
})
```

**Formato da resposta SSE**:
```
data: {"text": "palavra"}
data: {"text": " pr√≥xima"}
data: {"text": " palavra"}
data: [DONE]
```

### 2. Cliente (Extens√£o)
**Arquivo**: [src/js/content-simple.js](src/js/content-simple.js)

#### Fluxo Completo:

```
1. Usu√°rio envia pergunta
   ‚Üì
2. enviarMensagem() cria elemento de mensagem antecipadamente
   ‚Üì
3. gerarRespostaComContexto(texto, messageElement)
   ‚Üì
4. gerarRespostaIA(pergunta, messageElement)
   ‚Üì
5. openaiClient.analisarDocumento(contexto, pergunta, messageElement)
   ‚Üì
6. fazerRequisicao(prompt, messageElement)
   ‚Üì
7. Detecta Content-Type: text/event-stream
   ‚Üì
8. processarStreaming(response, messageElement)
   ‚Üì
9. Atualiza .lex-bubble em tempo real
```

#### Principais Modifica√ß√µes:

**1. Criar mensagem antecipadamente** ([content-simple.js:2814-2822](src/js/content-simple.js#L2814-L2822)):
```javascript
// Criar mensagem da IA antecipadamente (para streaming)
const assistantMessage = document.createElement('div');
assistantMessage.className = 'lex-message assistant';
assistantMessage.innerHTML = `
  <div class="lex-bubble"><span class="lex-thinking">Pensando...</span></div>
  <div class="lex-time">${new Date().toLocaleTimeString('pt-BR', { hour: '2-digit', minute: '2-digit' })}</div>
`;
messagesContainer.appendChild(assistantMessage);
```

**2. Passar elemento atrav√©s da cadeia de chamadas**:
- `gerarRespostaComContexto(pergunta, messageElement)`
- `gerarRespostaIA(pergunta, messageElement)`
- `analisarDocumento(contexto, pergunta, messageElement)`
- `fazerRequisicao(prompt, messageElement)`

**3. Detectar tipo de resposta** ([content-simple.js:494-516](src/js/content-simple.js#L494-L516)):
```javascript
const contentType = response.headers.get('Content-Type');

if (contentType && contentType.includes('text/event-stream')) {
  // üöÄ STREAMING HABILITADO
  return await this.processarStreaming(response, messageElement);
} else {
  // Fallback para resposta JSON (sem streaming)
  const data = await response.json();
  return data.resposta || ...;
}
```

**4. Processar streaming** ([content-simple.js:518-585](src/js/content-simple.js#L518-L585)):
```javascript
async processarStreaming(response, messageElement) {
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let fullText = '';

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value, { stream: true });
    const lines = chunk.split('\n');

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.substring(6);
        if (data === '[DONE]') continue;

        const parsed = JSON.parse(data);
        const text = parsed.text || '';

        if (text) {
          fullText += text;

          // Atualizar mensagem em tempo real
          if (messageElement) {
            const bubble = messageElement.querySelector('.lex-bubble');
            if (bubble) {
              bubble.innerHTML = this.limparResposta(fullText);

              // Auto-scroll
              const messagesContainer = messageElement.closest('.lex-messages');
              if (messagesContainer) {
                messagesContainer.scrollTop = messagesContainer.scrollHeight;
              }
            }
          }
        }
      }
    }
  }

  return fullText;
}
```

### 3. Estilos CSS
**Arquivo**: [styles/chat-styles.css:1851-1865](styles/chat-styles.css#L1851-L1865)

Anima√ß√£o do indicador "Pensando...":
```css
.lex-thinking {
  color: var(--lex-text-tertiary);
  font-style: italic;
  animation: lex-thinking-pulse 1.5s ease-in-out infinite;
}

@keyframes lex-thinking-pulse {
  0%, 100% { opacity: 0.4; }
  50% { opacity: 1; }
}
```

## Deploy

### 1. Deploy da Edge Function

```bash
# Fazer login no Supabase CLI (se ainda n√£o fez)
supabase login

# Navegar at√© a pasta do projeto
cd c:\Users\EDER\lex-test1

# Deploy da Edge Function com streaming
supabase functions deploy OPENIA --project-ref nspauxzztflgmxjgevmo
```

**Importante**: O arquivo `EDGE-FUNCTION-OPENIA-STREAMING.ts` deve substituir a Edge Function existente `OPENIA`.

### 2. Configurar Vari√°veis de Ambiente

Se ainda n√£o configurou, adicione a chave da OpenAI:

```bash
supabase secrets set OPENAI_API_KEY=sk-...
```

### 3. Recarregar a Extens√£o

1. Abra `chrome://extensions`
2. Encontre a extens√£o "Lex."
3. Clique no bot√£o üîÑ **Recarregar**
4. Aguarde at√© ver "Service worker (ativo)"

### 4. Testar

1. Abra uma p√°gina do PJe com um processo
2. Processe alguns documentos
3. Fa√ßa uma pergunta ao LEX
4. Observe a resposta aparecer palavra por palavra em tempo real

## Compatibilidade

### Fallback para JSON

A implementa√ß√£o √© **retrocompat√≠vel**. Se a Edge Function n√£o estiver retornando streaming (ou houver erro), o sistema automaticamente volta para o modo JSON:

```javascript
if (contentType && contentType.includes('text/event-stream')) {
  // Streaming
  return await this.processarStreaming(response, messageElement);
} else {
  // Fallback para JSON
  const data = await response.json();
  return data.resposta || ...;
}
```

### Estruturas de Resposta Suportadas

O sistema aceita m√∫ltiplos formatos de resposta JSON:
- `data.resposta`
- `data.response`
- `data.message`
- `data.fallback`
- `data.content`
- `data.text`

## Benef√≠cios do Streaming

### 1. **Percep√ß√£o de Velocidade**
- Usu√°rio v√™ as primeiras palavras **imediatamente**
- N√£o precisa esperar a resposta completa (que pode levar 10-30 segundos)
- Experi√™ncia similar ao ChatGPT

### 2. **Transpar√™ncia**
- Usu√°rio v√™ o progresso em tempo real
- Sabe que a IA est√° trabalhando
- Pode come√ßar a ler antes da conclus√£o

### 3. **Menor Lat√™ncia Percebida**
- TTFB (Time To First Byte) muito menor
- Primeira palavra em ~1-2 segundos
- vs. ~10-30 segundos no modo JSON

### 4. **Melhor UX**
- Indicador "Pensando..." animado no in√≠cio
- Texto aparece suavemente
- Auto-scroll acompanha o progresso

## Logs Esperados

### Console do Navegador

**Com Streaming (nova Edge Function)**:
```
üì§ LEX: Enviando requisi√ß√£o para Supabase Edge Function (streaming)...
üì• LEX: Status da resposta: 200
üì° LEX: Processando resposta com streaming...
‚úÖ LEX: Streaming conclu√≠do
üèÅ LEX: Recebido sinal de conclus√£o
‚úÖ LEX: Resposta da OpenAI recebida
```

**Sem Streaming (Edge Function antiga)**:
```
üì§ LEX: Enviando requisi√ß√£o para Supabase Edge Function (streaming)...
üì• LEX: Status da resposta: 200
üì¶ LEX: Processando resposta JSON (sem streaming)...
üì¶ LEX: Resposta da Edge Function: {...}
‚úÖ LEX: Resposta da OpenAI recebida
```

## Troubleshooting

### Problema: Streaming n√£o funciona

**Sintomas**: Resposta ainda aparece toda de uma vez

**Poss√≠veis causas**:
1. Edge Function antiga ainda ativa
2. Cache do navegador
3. Erro no deploy

**Solu√ß√£o**:
```bash
# 1. Verificar qual Edge Function est√° rodando
supabase functions list --project-ref nspauxzztflgmxjgevmo

# 2. Re-deploy for√ßado
supabase functions deploy OPENIA --project-ref nspauxzztflgmxjgevmo --no-verify-jwt

# 3. Limpar cache do navegador
Ctrl+Shift+Delete ‚Üí Limpar cache

# 4. Hard reload na extens√£o
chrome://extensions ‚Üí Recarregar
```

### Problema: Erro 500 na Edge Function

**Sintomas**: Console mostra "‚ùå LEX: Erro da Edge Function"

**Verificar**:
1. Logs da Edge Function:
```bash
supabase functions logs OPENIA --project-ref nspauxzztflgmxjgevmo
```

2. Vari√°vel de ambiente:
```bash
supabase secrets list --project-ref nspauxzztflgmxjgevmo
```

### Problema: Texto aparece truncado ou incorreto

**Sintomas**: Algumas palavras faltando ou duplicadas

**Causa**: Chunks SSE podem ser divididos no meio de uma linha

**Solu√ß√£o**: J√° implementada no c√≥digo - decodifica√ß√£o com `{ stream: true }`:
```javascript
const chunk = decoder.decode(value, { stream: true });
```

## Performance

### M√©tricas Estimadas

| M√©trica | Sem Streaming | Com Streaming |
|---------|---------------|---------------|
| TTFB (First Byte) | 1-3s | 1-2s |
| Primeira palavra vis√≠vel | 10-30s | 1-2s |
| Resposta completa | 10-30s | 10-30s |
| Percep√ß√£o de lat√™ncia | Alta | Baixa |

### Uso de Tokens

O streaming **n√£o aumenta** o uso de tokens. A quantidade de tokens enviados e recebidos √© **exatamente a mesma** do modo JSON.

### Largura de Banda

- **SSE**: ~10-20% mais dados (overhead do formato `data: {...}\n\n`)
- **Impacto**: Neglig√≠vel (alguns KB a mais por resposta)

## Pr√≥ximas Melhorias (Opcional)

### 1. Indicador de Typing
Mostrar "..." animado enquanto aguarda pr√≥ximo chunk:
```javascript
if (timeSinceLastChunk > 500) {
  bubble.innerHTML += '<span class="lex-typing">...</span>';
}
```

### 2. Cancelamento
Permitir usu√°rio cancelar resposta em progresso:
```javascript
const abortController = new AbortController();
fetch(url, { signal: abortController.signal });
// Cancelar: abortController.abort()
```

### 3. Retry Autom√°tico
Se streaming falhar, tentar novamente com JSON:
```javascript
try {
  return await processarStreaming(response);
} catch (error) {
  console.warn('Streaming falhou, usando JSON');
  return await processarJSON(response);
}
```

### 4. Buffer de Chunks
Acumular chunks pequenos antes de atualizar DOM (performance):
```javascript
let buffer = '';
if (buffer.length > 50 || done) {
  bubble.innerHTML += buffer;
  buffer = '';
}
```

## Refer√™ncias

- [OpenAI Streaming Guide](https://platform.openai.com/docs/api-reference/chat/create#chat/create-stream)
- [Server-Sent Events (MDN)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [ReadableStream API](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream)
- [Supabase Edge Functions](https://supabase.com/docs/guides/functions)

## Arquivos Modificados

### Novos Arquivos:
- [EDGE-FUNCTION-OPENIA-STREAMING.ts](EDGE-FUNCTION-OPENIA-STREAMING.ts) - Edge Function com streaming

### Arquivos Modificados:
- [src/js/content-simple.js](src/js/content-simple.js):
  - Linha 325: `analisarDocumento()` aceita `messageElement`
  - Linha 469: `fazerRequisicao()` detecta e processa streaming
  - Linha 518-585: Novo m√©todo `processarStreaming()`
  - Linha 2710: `gerarRespostaComContexto()` aceita `messageElement`
  - Linha 2814-2840: `enviarMensagem()` cria mensagem antecipadamente
  - Linha 3096: `gerarRespostaIA()` aceita `messageElement`

- [styles/chat-styles.css](styles/chat-styles.css):
  - Linha 1851-1865: CSS para indicador "Pensando..."

## Status

‚úÖ **Implementa√ß√£o Completa**
- Edge Function com streaming criada
- Cliente processando SSE
- Atualiza√ß√£o em tempo real do DOM
- Anima√ß√£o de "Pensando..."
- Fallback para JSON
- Auto-scroll
- Logs informativos

‚è≥ **Pendente**
- Deploy da Edge Function
- Teste em produ√ß√£o

## Pr√≥ximo Passo

**Deploy da Edge Function**:
```bash
cd c:\Users\EDER\lex-test1
supabase functions deploy OPENIA --project-ref nspauxzztflgmxjgevmo
```

Ap√≥s o deploy, o streaming estar√° ativo automaticamente! üöÄ
